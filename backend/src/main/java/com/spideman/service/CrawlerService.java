package com.spideman.service;

import com.spideman.entity.Article;
import com.spideman.entity.CrawlRecord;
import com.spideman.repository.CrawlRecordRepository;
import com.spideman.service.crawler.WebCrawler;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;

@Service
@RequiredArgsConstructor
@Slf4j
public class CrawlerService {
    
    private final List<WebCrawler> crawlers;
    private final ArticleService articleService;
    private final CrawlRecordRepository crawlRecordRepository;
    
    /**
     * ä»æ‰€æœ‰æºçˆ¬å–æ–‡ç« 
     */
    @Async
    public CompletableFuture<Map<String, Object>> crawlAllSources(int articlesPerSource) {
        Map<String, Object> result = new HashMap<>();
        int totalArticles = 0;
        int totalSuccess = 0;
        
        log.info("ğŸš€ å¼€å§‹ä»æ‰€æœ‰æºçˆ¬å–æ–‡ç« ï¼Œæ¯ä¸ªæº {} ç¯‡", articlesPerSource);
        
        for (WebCrawler crawler : crawlers) {
            if (!crawler.isAvailable()) {
                log.warn("çˆ¬è™« {} ä¸å¯ç”¨ï¼Œè·³è¿‡", crawler.getName());
                continue;
            }
            
            CrawlRecord record = new CrawlRecord();
            record.setSource(crawler.getSource());
            record.setCrawlTime(LocalDateTime.now());
            record.setStatus(CrawlRecord.CrawlStatus.RUNNING);
            
            try {
                log.info("ğŸ“¡ æ­£åœ¨çˆ¬å– {}...", crawler.getSource());
                
                List<Article> articles = crawler.crawlArticles(articlesPerSource);
                int successCount = 0;
                int errorCount = 0;
                
                for (Article article : articles) {
                    try {
                        articleService.saveArticle(article);
                        successCount++;
                        totalSuccess++;
                    } catch (Exception e) {
                        errorCount++;
                        log.warn("ä¿å­˜æ–‡ç« å¤±è´¥: {}", e.getMessage());
                    }
                    totalArticles++;
                }
                
                // æ›´æ–°çˆ¬å–è®°å½•
                record.setTotalCrawled(articles.size());
                record.setSuccessCount(successCount);
                record.setErrorCount(errorCount);
                record.setStatus(CrawlRecord.CrawlStatus.COMPLETED);
                
                log.info("âœ… {}: çˆ¬å– {} ç¯‡ï¼ŒæˆåŠŸ {} ç¯‡ï¼Œå¤±è´¥ {} ç¯‡", 
                    crawler.getSource(), articles.size(), successCount, errorCount);
                
            } catch (Exception e) {
                record.setStatus(CrawlRecord.CrawlStatus.FAILED);
                record.setErrorMessage(e.getMessage());
                log.error("âŒ {} çˆ¬å–å¤±è´¥: {}", crawler.getSource(), e.getMessage());
            } finally {
                crawlRecordRepository.save(record);
            }
        }
        
        result.put("totalAttempted", totalArticles);
        result.put("totalSuccess", totalSuccess);
        result.put("totalFailed", totalArticles - totalSuccess);
        result.put("crawlTime", LocalDateTime.now());
        
        log.info("ğŸ‰ çˆ¬å–å®Œæˆï¼æ€»è®¡: {} ç¯‡ï¼ŒæˆåŠŸ: {} ç¯‡", totalArticles, totalSuccess);
        return CompletableFuture.completedFuture(result);
    }
    
    /**
     * ä»æŒ‡å®šæºçˆ¬å–æ–‡ç« 
     */
    public Map<String, Object> crawlFromSource(String sourceName, int limit) {
        Map<String, Object> result = new HashMap<>();
        
        WebCrawler targetCrawler = crawlers.stream()
            .filter(crawler -> crawler.getSource().equalsIgnoreCase(sourceName))
            .findFirst()
            .orElse(null);
        
        if (targetCrawler == null) {
            result.put("success", false);
            result.put("message", "æœªæ‰¾åˆ°æŒ‡å®šçš„çˆ¬è™«æº: " + sourceName);
            return result;
        }
        
        CrawlRecord record = new CrawlRecord();
        record.setSource(targetCrawler.getSource());
        record.setCrawlTime(LocalDateTime.now());
        record.setStatus(CrawlRecord.CrawlStatus.RUNNING);
        
        try {
            log.info("ğŸ“¡ æ­£åœ¨ä» {} çˆ¬å– {} ç¯‡æ–‡ç« ...", sourceName, limit);
            
            List<Article> articles = targetCrawler.crawlArticles(limit);
            int successCount = 0;
            
            for (Article article : articles) {
                try {
                    articleService.saveArticle(article);
                    successCount++;
                } catch (Exception e) {
                    log.warn("ä¿å­˜æ–‡ç« å¤±è´¥: {}", e.getMessage());
                }
            }
            
            record.setTotalCrawled(articles.size());
            record.setSuccessCount(successCount);
            record.setErrorCount(articles.size() - successCount);
            record.setStatus(CrawlRecord.CrawlStatus.COMPLETED);
            
            result.put("success", true);
            result.put("source", sourceName);
            result.put("totalCrawled", articles.size());
            result.put("successCount", successCount);
            result.put("errorCount", articles.size() - successCount);
            
            log.info("âœ… {} çˆ¬å–å®Œæˆ: {} ç¯‡ï¼ŒæˆåŠŸ {} ç¯‡", sourceName, articles.size(), successCount);
            
        } catch (Exception e) {
            record.setStatus(CrawlRecord.CrawlStatus.FAILED);
            record.setErrorMessage(e.getMessage());
            
            result.put("success", false);
            result.put("message", "çˆ¬å–å¤±è´¥: " + e.getMessage());
            
            log.error("âŒ {} çˆ¬å–å¤±è´¥: {}", sourceName, e.getMessage());
        } finally {
            crawlRecordRepository.save(record);
        }
        
        return result;
    }
    
    /**
     * è·å–å¯ç”¨çš„çˆ¬è™«åˆ—è¡¨
     */
    public List<String> getAvailableCrawlers() {
        return crawlers.stream()
            .filter(WebCrawler::isAvailable)
            .map(WebCrawler::getSource)
            .collect(java.util.stream.Collectors.toList());
    }
    
    /**
     * è·å–çˆ¬å–ç»Ÿè®¡ä¿¡æ¯
     */
    public Map<String, Object> getCrawlStatistics() {
        Map<String, Object> stats = new HashMap<>();
        
        List<CrawlRecord> todayRecords = crawlRecordRepository.findTodayRecords();
        List<Object[]> sourceStats = crawlRecordRepository.getSourceStatistics();
        
        int todayCrawls = todayRecords.size();
        int todaySuccess = todayRecords.stream()
            .mapToInt(CrawlRecord::getSuccessCount)
            .sum();
        
        stats.put("todayCrawls", todayCrawls);
        stats.put("todaySuccess", todaySuccess);
        stats.put("availableCrawlers", getAvailableCrawlers());
        stats.put("sourceStatistics", sourceStats);
        
        return stats;
    }
} 